{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn Logistic Regression\n",
    "\n",
    "(C) 2024 by [Damir Cavar](http://damir.cavar.me/)\n",
    "\n",
    "**Version:** 1.3, October 2024\n",
    "\n",
    "**Download:** This and various other Jupyter notebooks are available from my [GitHub repo](https://github.com/dcavar/python-tutorial-notebooks).\n",
    "\n",
    "**License:** [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) ([CA BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/))\n",
    "\n",
    "This is a tutorial related to the discussion of training classifiers for antisemitism in social media detection at [Indiana University](https://www.iu.edu/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOC\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Learning Weights](#learning-weights)\n",
    "- [Gradient Descent](#gradient-descent)\n",
    "- [Using Scikit-Learn](#using-scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example problems are taken from the textbook Dan Jurafsky and James H. Martin (2023 draft) [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) chapter 5 on Logistic Regression. The code is written by [Damir Cavar](http://damir.cavar.me/) and simplified for use in the Advanced Natural Language Processing course taught at Indiana University in Fall 2023 and 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code we import all the used modules. You will need to make sure that [Scikit-learn](https://scikit-learn.org/stable/), and [NLTK](https://www.nltk.org/) are installed. You will need to implement a sigmoid function in a file called *secret.py* in the local folder. Since this is part of an assignment, this is not shared here yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "from secret import sigmoid\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import zipfile\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vader Lexicon file can be found in the NLTK data in `nltk-data/sentiment/vader_lexicon.zip`. It contains a list of tokens with sentiment ratings. Each line represents one token and the tab-seperated values are:\n",
    "- token\n",
    "- the mean of the human sentiment ratings\n",
    "- the Standard Deviation of the token\n",
    "- the list of 10 human ratings taken during experiments\n",
    "\n",
    "In the following the assumption is that the Vader lexicon is located in your `nltk-data`-folder. On Linux systems this is per default in your home directory. On Windows this is in your `AppData\\roaming` folder.\n",
    "\n",
    "We can read the Vader lexicon into a dictionary structure as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"damir\"\n",
    "# On Linux:\n",
    "# nltk_data_folder = f\"/home/{username}/nltk_data\"\n",
    "# On Windows:\n",
    "nltk_data_folder = f\"C:/Users/{username}/AppData/Roaming/nltk_data\"\n",
    "vader_filename = \"vader_lexicon/vader_lexicon.txt\"\n",
    "vader_data = {}\n",
    "with zipfile.ZipFile(os.path.join(nltk_data_folder, \"sentiment\", 'vader_lexicon.zip')) as z:\n",
    "    if vader_filename in z.namelist():\n",
    "        with z.open(vader_filename) as f:\n",
    "            for l in f:\n",
    "                tokens = l.decode(encoding='utf-8').strip().split('\\t')\n",
    "                if len(tokens) != 4:\n",
    "                    continue\n",
    "                vader_data[tokens[0]] = (float(tokens[1]), float(tokens[2]), ast.literal_eval(tokens[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now request the scores for existing tokens from the `vadar_data` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.6, 0.66332, [2, 3, 3, 3, 4, 3, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(vader_data[\"admirable\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assume that positive scores indicate that the token is typical for positive sentiment, while negative scores represent negative sentiment. We can see that for example when pulling the scores for token `annoying`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.7, 0.64031, [-1, -2, -1, -2, -1, -1, -2, -2, -3, -2])\n"
     ]
    }
   ],
   "source": [
    "print(vader_data[\"annoying\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the textbook the feature vector is generated using the following scores:\n",
    "- number of positive terms in text\n",
    "- number of negative terms\n",
    "- 1, if there is a *no* in the text, 0 if there is none\n",
    "- number of pronouns, all variants of 1st and 2nd person\n",
    "- 1 if there is a *!* in the text, 0 if there is none\n",
    "- the log of the number of tokens\n",
    "\n",
    "The following function generates a feature vector from some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_vector(text: str) -> list:\n",
    "    tokens = word_tokenize(text)\n",
    "    scores = [ vader_data.get(t, [0, 0]) for t in tokens ]\n",
    "    negative_terms = sum(1 for i in scores if i[0] < 0)\n",
    "    positive_terms = sum(1 for i in scores if i[0] > 0)\n",
    "    if \"no\" in tokens:\n",
    "        no_in_text = 1\n",
    "    else:\n",
    "        no_in_text = 0\n",
    "    pronouns = set( (\"I\", \"you\", \"me\", \"your\", \"mine\") )\n",
    "    count_pronouns = sum(1 for i in tokens if i in pronouns)\n",
    "    if \"!\" in tokens:\n",
    "        excl_in_text = 1\n",
    "    else:\n",
    "        excl_in_text = 0\n",
    "    return np.array([positive_terms, negative_terms, no_in_text, count_pronouns, excl_in_text, math.log(len(tokens))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use some sample text and generate a feature vector for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"It's hokey. There are virtually no surprises, and the writing is second-rate.\n",
    "So why was it so enjoyable? For one thing, the cast is great.\n",
    "Another nice touch is the music.\n",
    "I was overcome with the urge to get off the couch and start dancing.\n",
    "It sucked me in, and it'll do the same to you.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature vector is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.         2.         1.         3.         0.         4.21950771]\n"
     ]
    }
   ],
   "source": [
    "sample_text_vector = generate_feature_vector(sample_text)\n",
    "print(sample_text_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The textbook approach uses different vocabulary and entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_vector_textbook = np.array([3, 2, 1, 3, 0, 4.19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([2.5, -5.0, -1.2, 0.5, 2.0, 0.7])\n",
    "b = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the sigmoid scores for the feature vectors generated from the Vader lexicon is given in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_positive = sigmoid( np.dot(weights, sample_text_vector) + b )\n",
    "sigmoid_negative = 1 - sigmoid_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is classified as `positive sentiment` with 96% likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9662243326599138 0.03377566734008619\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid_positive, sigmoid_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the textbook scores for the text vector, our sigmoid values are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_positive = sigmoid( np.dot(weights, sample_text_vector_textbook) + b )\n",
    "sigmoid_negative = 1 - sigmoid_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is still judged as `positive sentiment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6969888901292717 0.3030111098707283\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid_positive, sigmoid_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Weights <a class=\"anchor\" id=\"learning-weights\"></a>\n",
    "\n",
    "The weights in the previos section have been manually set. In the following we will go over a strategy to learn those weights using the Cross-entropy Loss Function and Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the problem discussed in the textbook, the Cross-entropy Loss Function is defined as:\n",
    "\n",
    "$L_{CE}(\\hat{y},y) = -\\, log\\, p(y|x) = -\\, [y\\, log(\\hat{y}) + (1-y)\\, log(1-\\hat{y})] $\n",
    "\n",
    "If `y=1`, the second summand in the equation become `0`, thus we only look at:\n",
    "\n",
    "$L_{CE}(\\hat{y},1) = -\\, log\\, p(1|x) = -\\, 1\\, log(\\hat{y}) = -\\, log(\\hat{y})$\n",
    "\n",
    "For `y=0`, the first summand in the equation becomes `0`, this we only look at:\n",
    "\n",
    "$L_{CE}(\\hat{y},0) = -\\, log\\, p(0|x) = -\\, (1-0)\\, log(1-\\hat{y}) = -\\, log(1-\\hat{y}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_hat, y):\n",
    "  return -np.log(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y}$ is the `sigmoid` of the dot-product of the weight and feature vector after adding the bias value to it. That is, we can compute the `cross-entropy` from the `sigmoid` scores above\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid 0.6969888901292717 for y=1 Loss positive: 0.3609858079049309\n"
     ]
    }
   ],
   "source": [
    "cel_positive = cross_entropy_loss(sigmoid_positive, 1)\n",
    "print(f\"sigmoid {sigmoid_positive} for y={1} Loss positive: {cel_positive}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid 0.3030111098707283 for y=0 Loss positive: 1.1939858079049306\n"
     ]
    }
   ],
   "source": [
    "cel_negative = cross_entropy_loss(sigmoid_negative, 0)\n",
    "print(f\"sigmoid {sigmoid_negative} for y={0} Loss positive: {cel_negative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent <a class=\"anchor\" id=\"gradient-descent\"></a>\n",
    "\n",
    "Loss function is paretrized by the weights $\\theta = (w,b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"\"\"It's hokey. There are virtually no surprises, and the writing is second-rate.\n",
    "So why was it so enjoyable? For one thing, the cast is great.\n",
    "Another nice touch is the music.\n",
    "I was overcome with the urge to get off the couch and start dancing.\n",
    "It sucked me in, and it'll do the same to you.\"\"\", 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stochastic gradient descent function in our simple classification task can be defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(data):\n",
    "    w = np.array([0, 0, 0, 0, 0, 0])\n",
    "    b = 0\n",
    "    learning_rate = 0.1\n",
    "    for text, y in data:\n",
    "        x = generate_feature_vector(text)\n",
    "        print(\"x:\", x)\n",
    "        y_hat = sigmoid( np.dot(w, x) + b )\n",
    "        print(\"y_hat:\", y_hat)\n",
    "        gradient_b = y_hat - y\n",
    "        print(\"gradient b:\", gradient_b)\n",
    "        b = b - learning_rate * gradient_b\n",
    "        print(\"new gradient b:\", b)\n",
    "        gradient_w = (y_hat - y) * x\n",
    "        print(\"gradient w:\", gradient_w)\n",
    "        # w = gradient_w - learning_rate * gradient_w\n",
    "        w = w - learning_rate * gradient_w\n",
    "        print(\"new gradient w:\", w)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [4.         2.         1.         3.         0.         4.21950771]\n",
      "y_hat: 0.5\n",
      "gradient b: -0.5\n",
      "new gradient b: 0.05\n",
      "gradient w: [-2.         -1.         -0.5        -1.5        -0.         -2.10975385]\n",
      "new gradient w: [0.2        0.1        0.05       0.15       0.         0.21097539]\n"
     ]
    }
   ],
   "source": [
    "w, b = stochastic_gradient_descent(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_silent(data):\n",
    "    w = np.array([0, 0, 0, 0, 0, 0])\n",
    "    b = 0\n",
    "    learning_rate = 0.1\n",
    "    for text, y in data:\n",
    "        x = generate_feature_vector(text)\n",
    "        y_hat = sigmoid( np.dot(w, x) + b )\n",
    "        gradient_b = y_hat - y\n",
    "        b = b - learning_rate * gradient_b\n",
    "        gradient_w = (y_hat - y) * x\n",
    "        w = w - learning_rate * gradient_w\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('.', 'data', 'reviews.csv'), newline='') as csvfile:\n",
    "    datareader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    header = next(datareader)\n",
    "    for row in datareader:\n",
    "        if len(row) == 2:\n",
    "            experiment_data.append( [row[0].strip(), int(row[1].strip())] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 25000\t Negative: 25000\n",
      "Total reviews: 50000\n"
     ]
    }
   ],
   "source": [
    "count_positive = sum([ 1 for x in experiment_data if x[1] == 1 ])\n",
    "count_negative = sum([ 1 for x in experiment_data if x[1] == 0 ])\n",
    "print(f\"Positive: {count_positive}\\t Negative: {count_negative}\")\n",
    "print(\"Total reviews:\", len(experiment_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\", 0], [\"This is an example of why the majority of action films are the same. Generic and boring, there's really nothing worth watching here. A complete waste of the then barely-tapped talents of Ice-T and Ice Cube, who've each proven many times over that they are capable of acting, and acting well. Don't bother with this one, go see New Jack City, Ricochet or watch New York Undercover for Ice-T, or Boyz n the Hood, Higher Learning or Friday for Ice Cube and see the real deal. Ice-T's horribly cliched dialogue alone makes this film grate at the teeth, and I'm still wondering what the heck Bill Paxton was doing in this film? And why the heck does he always play the exact same character? From Aliens onward, every film I've seen with Bill Paxton has him playing the exact same irritating character, and at least in Aliens his character died, which made it somewhat gratifying... Overall, this is second-rate action trash. There are countless better films to see, and if you really want to see this one, watch Judgement Night, which is practically a carbon copy but has better acting and a better script. The only thing that made this at all worth watching was a decent hand on the camera - the cinematography was almost refreshing, which comes close to making up for the horrible film itself - but not quite. 4/10.\", 0], [\"First of all I hate those moronic rappers, who could'nt act if they had a gun pressed against their foreheads. All they do is curse and shoot each other and acting like clichÃ©'e version of gangsters. The movie doesn't take more than five minutes to explain what is going on before we're already at the warehouse There is not a single sympathetic character in this movie, except for the homeless guy, who is also the only one with half a brain. Bill Paxton and William Sadler are both hill billies and Sadlers character is just as much a villain as the gangsters. I did'nt like him right from the start. The movie is filled with pointless violence and Walter Hills specialty: people falling through windows with glass flying everywhere. There is pretty much no plot and it is a big problem when you root for no-one. Everybody dies, except from Paxton and the homeless guy and everybody get what they deserve. The only two black people that can act is the homeless guy and the junkie but they're actors by profession, not annoying ugly brain dead rappers. Stay away from this crap and watch 48 hours 1 and 2 instead. At lest they have characters you care about, a sense of humor and nothing but real actors in the cast.\", 0]]\n",
      "[0.2        0.1        0.05       0.15       0.         0.21097539] 0.05\n"
     ]
    }
   ],
   "source": [
    "print(experiment_data[:3])\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = stochastic_gradient_descent_silent(experiment_data[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.4936382  -0.43017071 -0.05020146 -0.16100752 -0.0022494  -0.32810755] -0.06373438506975194\n"
     ]
    }
   ],
   "source": [
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data, w, b):\n",
    "    res = []\n",
    "    for text, y in data:\n",
    "        x = generate_feature_vector(text)\n",
    "        y_hat = sigmoid( np.dot(w, x) + b )\n",
    "        if y_hat > .5:\n",
    "            y_hat = 1\n",
    "        else:\n",
    "            y_hat = 0\n",
    "        res.append( (y, y_hat) )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(experiment_data[-20:], w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(1, 0): 20})\n"
     ]
    }
   ],
   "source": [
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn <a class=\"anchor\" id=\"using-scikit-learn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the data from the reviews data set above. For that we will create two lists containing the text and the label respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data_labels = []\n",
    "for e in experiment_data:\n",
    "    data.append(e[0])\n",
    "    if e[1] == 1:\n",
    "        data_labels.append('pos')\n",
    "    else:\n",
    "        data_labels.append('neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be transformed into feature vectors that take frequency into account, remove function words, and so on. The tokens or text is not normalized to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = 'word', lowercase = False,)\n",
    "features = vectorizer.fit_transform(data)\n",
    "features_nd = features.toarray() # for easy usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the corpus into a training and test corpus, using 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        features_nd, \n",
    "        data_labels,\n",
    "        train_size=0.80, \n",
    "        random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\damir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the prediction on the 20% test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the results for a few random examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "This movie should go down in the annals of fiefdom as one of the worst of all time. I will stop short of saying it's the worst movie ever, only because I have yet to see every movie ever made. I cannot make such lofty claims until then. The story is stale, the acting is horrible, at best, the \"special\" effects are no more than a couple of lbs. of dry ice and a fan. Somebody must have been related to someone to get this movie made. Mr. Busey mailed this one in! The dog is well trained and cute, making it the only redeeming quality in this never-should-have-made-it movie. Two hours and $3 of my life I will never get back.\n",
      "pos\n",
      "Okay, make no mistake - this is a pretty awful film, but I actually thought it had a couple of creepy scenes and overcame its pathetic budget every now and then. At the very least it's unintentionally funny in spots and has a definite air of creepiness and discomfort (a face burning scene, the part with the disfigured bride).  This baby falls into the \"so bad it's entertaining\" category to me, and for that alone I would give it a star. The effects are terrible, the acting is abysmal, and the whole thing looks like it was shot in a day. You gotta love that toy ship at the beginning, too! It brought back childhood memories of seeing this on late night TV many years ago. While the Alpha DVD print looks weak and as though it was recorded directly off an old television broadcast or something, I actually liked that in this case!\n",
      "pos\n",
      "This movie is a ripoff of James Cain's novel, THE POSTMAN ALWAYS RINGS TWICE. Apparently, the director and producer never bothered to pay for rights to this story--perhaps the fact that we were in the middle of fighting the Italians in WWII might account for their forgetting to consider royalties! Despite this, the movie isn't really just an Italian version of the Hollywood movie. In some ways it's a lot better and in other ways, it is definitely not. The three central characters in this movie are really pretty ugly people. In fact, the male and female lovers are a bit icky-looking. The male lead is pretty ordinary except for his profuse body hair (particularly on the back and shoulders) and his lady love is, to put it frankly, unattractive. They are a very, very far cry from Lana Turner and John Garfield in the Hollywood version. And the ill-fated husband is really, really obese and loves to walk around shirtless--and his counterpart in the American film, Cecil Kellaway is definitely better looking (and probably better looking than the other two Italian leads, actually). And this unattractiveness is generally a reason I actually preferred the Italian film--since I just could NOT imagine a finely coiffed \"dish\" like Lana Turner in the middle of nowhere married to Kellaway--I am 100% sure she would have had dozens of better offers! Whereas, the Italian wife frankly might NOT have been able to do much better and this made the marriage actually believable. Part of the Italian film's believability comes from the blunt way it handles sex. The sanitized American film tries to make you believe that although Turner and Garfield kill Kellaway, they never actually get around to sex! This is pretty silly and totally unrealistic. In addition to the casual sexuality of the film, it's also pretty casual in showing the seamy side of life--with lots of sweaty people, a fly strip hanging over the kitchen table and everyone appeared to need a bath. The movie is also pretty fast-paced compared to the over-long American film. And what you get due to brevity isn't all good. The film lacks a lot of the style and polish of the American film--with grainier footage, relatively poor orchestration and sets. It sure ain't a pretty film, but the Neo-Realistic-like style makes the film seem more realistic. But it cannot make up for the short-cuts in the plot. Many of the plot elements in the later American version are either missing entirely or glossed over. And the ending seems a lot less interesting than the American film--and misses the entire human nature dilemma when Turner and Garfield turn on each other like rats (the best part of the American film). So which is the better film? Well, a lot of this probably depends on you. As for me, the Warner Brothers film was simply too polished and too unrealistic (though many like this style and may dislike watching films with subtitles)--but it packed a great ending. And the Italian film was much, much more realistic--until the crappy ending that seemed too rushed. So neither film is exactly great, but I'd give my nod to the Italian one being a bit better. It's too bad they couldn't have combined the best elements of both films into one exceptional film.\n",
      "pos\n",
      "The last sentence of this review is a major spoiler. I have enjoyed Joe Dante's work since Piranha. He's done a great deal of different genre parodies that were both funny and honest. But this is pure crap. This is the kind of satire - in line with Thank You For Smoking - that is so literal and direct that it leaves nothing a) to be laughed at and b) to leave the audience to think about. It's a shame, because the plot and the material is so rich, timely and ripe for intelligent commentary. By the way, there is absolutely no reason for the main character to shoot the Ann Coulter character at the end of the film. It's just flat out ridiculous.\n",
      "pos\n",
      "The first step to getting off of that road that leads to nowhere is recognizing that you're on it in the first place; then it becomes a matter of being assertive and taking positive steps to overcome the negative influences in your life that may have put you on that road to begin with. Which is exactly what a young Latino girl does in `Girlfight,' written and directed by Karyn Kusama. Diana (Michelle Rodriguez) is an eighteen-year-old High School senior from the projects in Brooklyn, facing expulsion after her fourth fight in the halls since the beginning of the semester. She affects a `whatever' attitude which masks a deep-seated anger that threatens to take her into places she'd rather not go. She lives with her father, Sandro (Paul Calderon), with whom she has a very tentative relationship, and her younger brother, Tiny (Ray Santiago). With her life teetering on the brink of dissolution, she desperately needs an outlet through which to channel the demons that plague her. And one day she finds it, without even looking for it, when she stops by the gym where Tiny trains. Ironically, Tiny wants nothing to do with boxing; he wants to go to art school, but Sandro is determined that his son should be able to take care of himself on the streets, and pays the ten dollars a week it costs for his lessons. When Diana convinces Tiny's trainer, Hector (Jaime Tirelli), to take her on, and approaches her father for the money, under the guise of calling it a weekly allowance (she doesn't want him to know what she wants the money for), Sandro turns her down and tells her to go out and earn her own money. Ultimately, with Tiny's help she finds a way, and the ring soon becomes her second home. It's an environment to which she readily adapts, and it appears that her life is about to take a turn for the better. And the fact that she will have to fight men, not women, in `gender blind' competitions, does not faze her in the least. Diana has found her element. \tFirst time writer/director Karyn Kusama has done a terrific job of creating a realistic setting for her story, presenting an honest portrait of life in the projects and conveying that desperation so familiar to so many young people who find themselves in dead-end situations and on that road that leads to nowhere. And there's no candy coating on it, either; as Hector tells Diana when she asks him how he came to be where he is, `I was a fighter once. I lost.' Then, looking around the busy gym, `Like most of these guys, they're going to lose, too. But it's all they know--' And it's that honesty of attitude, as well as the way in which the characters are portrayed, that makes this movie as good as it is. It's a bleak world, underscored by the dimly lit, run-down gym-- you can fairly smell the sweat of the boxers-- and that sense of desolation that hangs over it all like a pall, blanketing these people who are grasping and hanging on to the one and only thing they have, all that they know. \tMaking her screen debut, Michelle Rodriguez is perfectly cast as Diana, infusing her with a depth and brooding intensity that fairly radiates off of her in waves. She is so real that it makes you wonder how much of it is really Rodriguez; exactly where does the actor leave off and the character begin? Whatever it is, it works. It's a powerful, memorable performance, by an actor from whom we will await another endeavor with great anticipation. She certainly makes Diana a positive role model, one in whom many hopefully will find inspiration and the realization that there are alternative paths available in life, at least to those who would seek them out. \tAs positive as this film is, however, it ends on something of an ambiguous note; though Diana obviously has her feet on the ground, there's no indication of where she's headed. Is this a short term fix for her, or is she destined to become the female counterpart of Hector? After all, realistically (and in light of the fact that the realism is one of the strengths of this film), professional boxing isn't exactly a profession that lends itself to, nor opens it's arms to women. And in keeping with the subject matter of the film, and the approach of the filmmaker, an affirmation of the results of Diana's assertiveness would have been appropriate. \tThe supporting cast includes Santiago Douglas (Adrian), Elisa Bocanegra (Marisol), Alicia Ashley (Ricki) and Thomas Barbour (Ira). Though it delivers a very real picture of life to which many will be able to identify, there are certain aspects of `Girlfight,' that stretch credibility a bit, regarding some of what happens in the ring. That aside, it's a positive film that for the most part is a satisfying experience. I rate this one 7/10.\n",
      "pos\n",
      "Having seen 'only' about 200 Hong Kong films in my time, I have to say this film is among my very top favorites. Not only is the plot engaging (and in some ways surprising, which these days is rare for any movie), but the chemistry between the two lead actors is superb. Top notch casting! And while often even the most serious HK films tend to insert quite a bit of humor in between all the drama and action, often spoiling the mood a bit, here the jokes are kept subtle and woven into the plot, even improving character relations. The music is also very well done, and the two main themes are very beautiful. With the release of the HK special Edition, they've even cleaned the picture (first release was grainy) and the subtitles, even if the quality of the translation is still lacking (nothing new there). All in all, if you have to see a HK film that isn't directed by John Woo or have Chow Yun Fat in it, this should be at least on your short list! A truly fascinating and entertaining watch!\n",
      "pos\n",
      "OK, as everyone has pointed out, this film is a complete dog. To some degree this is because it was a gory sexploitation film that had a lot of material excised (or darkened down to near invisibility) to escape the censor's X-rating; but the film has many other flaws as well. To begin with, the scriptwriter seems to have got his werewolves and vampires mixed up. The baddies in this film are furry and don't like silver but in every other respect they behave like vampires. Now you just can't do that with a crappy genre flick, you've got to stick to the rules of the genre or the fans get all confused and annoyed by suspending disbelief in the wrong thing. In fact the whole (confusing and poorly presented) plot is something that has already been done for vampires, but doesn't make any sense in a werewolf movie. Secondly, the werewolf costumes are the lamest you have ever seen. Anybody in the werewolf movie business ought to know that the werewolf costumes and transformations are something the fans assess critically, yet some of these werewolves are just plain goofy. There are a couple of slightly good bits. I actually quite liked the score. Others have mentioned Sybil Danning's tits. And... (***SPOILER***, if such a thing can exist) I also quite liked the plan for attacking the werewolves' stronghold. There are so many horror movies that rely on characters behaving stupidly, but in this case they first acquire a very sensible and effective anti-werewolf arsenal and go slaughter the monsters. I mean, you can kill werewolves with silver bullets, and we have some pretty powerful firearms these days. Shouldn't be too hard to put two and two together, hmm? But in typical style this movie goes over the top and adds some other very zany and amusing anti-lycanthrope weapons.\n"
     ]
    }
   ],
   "source": [
    "j = random.randint(0,len(X_test)-7)\n",
    "for i in range(j,j+7):\n",
    "    print(y_pred[0])\n",
    "    ind = features_nd.tolist().index(X_test[i].tolist())\n",
    "    print(data[ind].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the overall accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8875\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(C) 2024 by [Damir Cavar](http://damir.cavar.me/) <<dcavar@iu.edu>>**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
